ğŸ“‚ KODTRÃ„D
==========
â”œâ”€â”€ Documents
â”‚   â”œâ”€â”€ KLR_AI
â”‚   â”‚   â”œâ”€â”€ Projekt_MacSpot
â”‚   â”‚   â”‚   â”œâ”€â”€ macspot-api
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ AllSyncCodes_history
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ _lokal_temp.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ§Š_azure_temp.txt
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ§®_diff_output.txt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ healthcheck_sync.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sync.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sync_all.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sync_from_cloud.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sync_plist.xml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sync_static_tables.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ sync_to_cloud.py
â”œâ”€â”€ Library
â”‚   â”œâ”€â”€ LaunchAgents
â”‚   â”‚   â”œâ”€â”€ com.macspot.sync.plist
==========

====================
ğŸ“„ Fil: Documents/KLR_AI/Projekt_MacSpot/macspot-api/sync_from_cloud.py
ğŸ“‚ Kodtyp: ğŸ“„ Ã–vrigt
ğŸ—‚ Filtyp: ğŸ Python
ğŸ“… Senast Ã¤ndrad: 2025-06-02 17:06:13
ğŸ“ Antal rader: 161
ğŸ§© Antal funktioner: 0
ğŸ’¬ KommentarstÃ¤ckning: 0 rader (0.0%)
ğŸ“¥ Imports: 2 â€“ ['import psycopg2', 'import json']
ğŸ” LÃ¤ngsta funktion: 0 rader
ğŸ§  KomplexitetspoÃ¤ng: 22
ğŸ§ª TODO/FIXME: 0
====================
START: sync_from_cloud.py
import psycopg2
import json
from config import LOCAL_DB_CONFIG, REMOTE_DB_CONFIG

def connect_db(config):
    return psycopg2.connect(**config)

def safe_json_load(data, default={}):
    try:
        return json.loads(data) if isinstance(data, str) else data
    except Exception:
        return default

def metadata_equal(meta1, meta2):
    m1 = safe_json_load(meta1)
    m2 = safe_json_load(meta2)
    return m1 == m2

def apply_change(cur, table, operation, payload):
    try:
        if operation == "INSERT":
            cols = ", ".join(payload.keys())
            placeholders = ", ".join(["%s"] * len(payload))
            sql = f"INSERT INTO {table} ({cols}) VALUES ({placeholders}) ON CONFLICT (id) DO NOTHING"
            cur.execute(sql, [json.dumps(v) if isinstance(v, dict) else v for v in payload.values()])

        elif operation == "UPDATE":
            if table == "contact" and "metadata" in payload:
                local_meta = payload["metadata"]
                cur.execute("SELECT metadata FROM contact WHERE id = %s", (payload["id"],))
                row = cur.fetchone()
                if row and metadata_equal(row[0], local_meta):
                    print(f"â™»ï¸ Ingen skillnad i metadata fÃ¶r {payload['id']}, hoppar UPDATE.")
                    return
                else:
                    cur.execute("""
                        INSERT INTO event_log (id, source, event_type, payload, received_at)
                        VALUES (gen_random_uuid(), %s, %s, %s, now())
                    """, (
                        'sync',
                        'sync_fromcloud_mismatch',
                        json.dumps({
                            "record_id": str(payload["id"]),
                            "email": payload.get("booking_email"),
                            "metadata_before": row[0],
                            "metadata_after": local_meta,
                            "diff_summary": [k for k in local_meta if local_meta.get(k) != row[0].get(k)]
                        })
                    ))

            sets = ", ".join([f"{col} = %s" for col in payload if col != "id"])
            values = [json.dumps(payload[col]) if isinstance(payload[col], dict) else payload[col]
                      for col in payload if col != "id"]
            values.append(payload["id"])
            sql = f"UPDATE {table} SET {sets} WHERE id = %s"
            cur.execute(sql, values)

            # Verifiera resultat (endast fÃ¶r kontakt)
            if table == "contact":
                cur.execute("SELECT metadata, updated_at FROM contact WHERE id = %s", [payload["id"]])
                updated_row = cur.fetchone()
                if updated_row:
                    try:
                        metadata = safe_json_load(updated_row[0])
                        address = metadata.get("address") or metadata.get("company") or "(ingen adress)"
                    except Exception:
                        address = "(kunde inte tolkas)"
                    print(f"ğŸ§¾ Efter UPDATE: {payload['id']} â†’ {address} @ {updated_row[1]}")
                else:
                    print(f"âš ï¸ UPDATE-verifiering misslyckades: Inget resultat fÃ¶r {payload['id']}")

        elif operation == "DELETE":
            cur.execute(f"DELETE FROM {table} WHERE id = %s", [payload["id"]])
            print(f"ğŸ—‘ï¸ Raderade post {payload['id']} frÃ¥n {table}")
            print(f"ğŸ—‘ï¸ Raderade post {payload['id']} frÃ¥n {table}")

    except Exception as e:
        print(f"âŒ Fel i apply_change fÃ¶r {table} ({operation}): {e}")
        raise

def sync():
    remote_conn = connect_db(REMOTE_DB_CONFIG)
    remote_cur = remote_conn.cursor()

    local_conn = connect_db(LOCAL_DB_CONFIG)
    local_cur = local_conn.cursor()

    try:
        remote_cur.execute("""
            SELECT id, table_name, record_id, operation, payload
            FROM (
                SELECT *, ROW_NUMBER() OVER (PARTITION BY record_id ORDER BY created_at ASC) AS rn
                FROM pending_changes
                WHERE direction = 'out' AND processed = false
                  AND table_name IN ('contact', 'bookings')
            ) sub
            WHERE rn = 1
            ORDER BY created_at ASC, id
        """)
        rows = remote_cur.fetchall()

        remote_cur.execute("""
            DELETE FROM pending_changes
            WHERE id NOT IN (
                SELECT id FROM (
                    SELECT id, ROW_NUMBER() OVER (PARTITION BY record_id ORDER BY created_at ASC) AS rn
                    FROM pending_changes
                    WHERE direction = 'out' AND processed = false
                ) sub
                WHERE rn = 1
            ) AND direction = 'out' AND processed = false AND operation = 'UPDATE';
        """)

        count = 0
        for row in rows:
            change_id, table, record_id, operation, payload_json = row
            try:
                payload = safe_json_load(payload_json)
                if not isinstance(payload.get("id"), str) or "your-generated-id" in payload.get("id"):
                    continue

                apply_change(local_cur, table, operation, payload)

                if table == "bookings" and operation == "INSERT":
                    local_cur.execute("""
                        UPDATE pending_changes
                        SET booking_id = %s
                        WHERE record_id = %s AND table_name = 'bookings' AND booking_id IS NULL
                    """, (record_id, record_id))

                # Logga kontaktimport
                if table == "contact":
                    email = payload.get("booking_email", "(okÃ¤nd e-post)")
                    meta = safe_json_load(payload.get("metadata", {}))
                    address = meta.get("address", "(okÃ¤nd adress)")
                    print(f"ğŸ“¥ Importerad kontakt: {email} â†’ {address}")

                local_cur.execute("""
                    INSERT INTO event_log (id, source, event_type, payload, received_at)
                    VALUES (gen_random_uuid(), %s, %s, %s, now())
                """, ('sync', f"{operation.lower()}_{table}", json.dumps(payload)))

                remote_cur.execute("UPDATE pending_changes SET processed = true WHERE id = %s", [change_id])
                remote_conn.commit()
                local_conn.commit()
                count += 1

            except Exception as e:
                print(f"âŒ Fel vid synk fÃ¶r {table} (id={change_id}): {e}")
                local_conn.rollback()
                remote_conn.rollback()
                continue

    finally:
        local_cur.close()
        remote_cur.close()
        local_conn.close()
        remote_conn.close()

if __name__ == "__main__":
    sync()
END: sync_from_cloud.py

====================
ğŸ“„ Fil: Documents/KLR_AI/Projekt_MacSpot/macspot-api/sync_to_cloud.py
ğŸ“‚ Kodtyp: ğŸ“„ Ã–vrigt
ğŸ—‚ Filtyp: ğŸ Python
ğŸ“… Senast Ã¤ndrad: 2025-06-02 17:26:31
ğŸ“ Antal rader: 283
ğŸ§© Antal funktioner: 0
ğŸ’¬ KommentarstÃ¤ckning: 0 rader (0.0%)
ğŸ“¥ Imports: 3 â€“ ['import psycopg2', 'import json', 'import traceback']
ğŸ” LÃ¤ngsta funktion: 0 rader
ğŸ§  KomplexitetspoÃ¤ng: 45
ğŸ§ª TODO/FIXME: 0
====================
START: sync_to_cloud.py
import psycopg2
import json
from datetime import datetime, timezone
from config import LOCAL_DB_CONFIG, REMOTE_DB_CONFIG

def safe_json_load(data, default={}):
    try:
        return json.loads(data) if isinstance(data, str) else data
    except Exception:
        return default

def metadata_equal(meta1, meta2):
    m1 = safe_json_load(meta1)
    m2 = safe_json_load(meta2)
    return m1 == m2

def connect_db(config):
    return psycopg2.connect(**config)

def fetch_pending_changes(conn):
    with conn.cursor() as cur:
        cur.execute("""
            SELECT id, table_name, record_id, operation, payload
            FROM (
                SELECT *, ROW_NUMBER() OVER (PARTITION BY record_id ORDER BY created_at ASC) AS rn
                FROM pending_changes
                WHERE direction = 'out' AND processed = false
                  AND table_name IN ('contact', 'bookings')
            ) sub
            WHERE rn = 1
            ORDER BY created_at ASC, id
        """)
        rows = cur.fetchall()

        # Rensa Ã¤ldre UPDATE-poster med samma record_id
        cur.execute("""
            DELETE FROM pending_changes
            WHERE id NOT IN (
                SELECT id FROM (
                    SELECT id, ROW_NUMBER() OVER (PARTITION BY record_id ORDER BY created_at ASC) AS rn
                    FROM pending_changes
                    WHERE direction = 'out' AND processed = false
                ) sub
                WHERE rn = 1
            ) AND direction = 'out' AND processed = false AND operation = 'UPDATE';
        """)
        return rows

def mark_as_processed(conn, change_id):
    with conn.cursor() as cur:
        cur.execute("UPDATE pending_changes SET processed = true WHERE id = %s", (change_id,))
        conn.commit()

#
# ğŸ“ SYNC-BETEENDE: Hantering av metadata
#
# Viktigt att fÃ¶rstÃ¥ skillnaden:
#
# 1. Ã„ndring av vÃ¤rde:
#    - Exempel: "postal_code": "111 11" â†’ "115 32"
#    - Hanteras som en vanlig UPDATE (om updated_at Ã¤r nyare)
#
# 2. Ã„ndring av nyckel (etikett):
#    - Exempel: "postal_number" â†’ "postal_code"
#    - Molnet kommer *inte* ta bort "postal_number" utan force_resync
#    - LÃ¤gg till `"force_resync": true` i metadata fÃ¶r att tvinga full Ã¶verskrivning
#
# Detta minskar risken att data i molnet raderas av misstag.

def apply_change(conn, change, local_conn):
    table_name, record_id, operation, payload = change[1], change[2], change[3], change[4]
    with conn.cursor() as cur:
        data = safe_json_load(payload)

        # Skip contact records with metadata.origin != 'klrab.se'
        if table_name == 'contact' and 'metadata' in data:
            meta = safe_json_load(data['metadata'])
            if meta.get('origin') != 'klrab.se':
                print(f"âš ï¸ Skickas ej: origin != klrab.se â€“ {data.get('booking_email')}")
                mark_as_processed(local_conn, change[0])
                return

        # Ensure all values are serializable to SQL
        for k, v in data.items():
            if isinstance(v, dict):
                data[k] = json.dumps(v)

        if 'updated_at' in data:
            if isinstance(data['updated_at'], str):
                # Parse and convert to UTC if it's a string
                try:
                    dt = datetime.fromisoformat(data['updated_at'])
                    data['updated_at'] = dt.astimezone(timezone.utc).isoformat()
                except Exception as e:
                    print(f"âš ï¸ Kunde inte tolka updated_at: {data['updated_at']} ({e})")
            elif isinstance(data['updated_at'], datetime):
                data['updated_at'] = data['updated_at'].astimezone(timezone.utc).isoformat()

        if operation == 'INSERT':
            columns = ', '.join(data.keys())
            placeholders = ', '.join(['%s'] * len(data))
            values = list(data.values())
            cur.execute(
                f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders}) "
                f"ON CONFLICT (id) DO UPDATE SET "
                f"{', '.join([f'{k} = EXCLUDED.{k}' for k in data.keys() if k != 'id'])}",
                values
            )
            if table_name == 'bookings':
                with local_conn.cursor() as local_cur:
                    local_cur.execute(
                        """
                        UPDATE pending_changes
                        SET booking_id = %s
                        WHERE record_id = %s AND table_name = 'bookings' AND booking_id IS NULL
                        """,
                        (record_id, record_id)
                    )
                    local_conn.commit()

        if data.get("force_resync") is True:
            print(f"ğŸ” Force resync aktiv â€“ uppdaterar {table_name} {record_id}")

        if operation == 'UPDATE':
            # Merge metadata with existing remote value and ensure JSON string (only for UPDATE)
            if 'metadata' in data and table_name == 'contact':
                cur.execute(f"SELECT metadata FROM {table_name} WHERE id = %s", (record_id,))
                row = cur.fetchone()
                if row and row[0]:
                    existing_metadata = safe_json_load(row[0])
                else:
                    existing_metadata = {}

                incoming_metadata = safe_json_load(data['metadata'])
                if metadata_equal(existing_metadata, incoming_metadata):
                    mark_as_processed(local_conn, change[0])
                    return
                existing_metadata.update(incoming_metadata)
                changed_keys = [k for k in incoming_metadata if existing_metadata.get(k) != incoming_metadata[k]]
                if not changed_keys:
                    mark_as_processed(local_conn, change[0])
                    return
                data['metadata'] = json.dumps(existing_metadata)

            # FÃ¶rbÃ¤ttrad hantering av tidsjÃ¤mfÃ¶relse fÃ¶r updated_at
            if 'updated_at' in data and not data.get("force_resync"):
                try:
                    # SÃ¤kerstÃ¤ll att local_ts Ã¤r datetime i UTC
                    local_ts = data['updated_at']
                    if isinstance(local_ts, str):
                        local_ts = datetime.fromisoformat(local_ts)
                    if local_ts.tzinfo is None:
                        local_ts = local_ts.replace(tzinfo=timezone.utc)
                    else:
                        local_ts = local_ts.astimezone(timezone.utc)

                    cur.execute(f"SELECT updated_at FROM {table_name} WHERE id = %s", (record_id,))
                    row = cur.fetchone()
                    if row and row[0] and isinstance(row[0], datetime):
                        remote_ts = row[0]
                        if remote_ts.tzinfo is None:
                            remote_ts = remote_ts.replace(tzinfo=timezone.utc)
                        else:
                            remote_ts = remote_ts.astimezone(timezone.utc)

                        if local_ts <= remote_ts:
                            if 'metadata' in data and table_name == 'contact':
                                cur.execute(f"SELECT metadata FROM {table_name} WHERE id = %s", (record_id,))
                                meta_row = cur.fetchone()
                                if meta_row and not metadata_equal(meta_row[0], data['metadata']):
                                    print(f"âš ï¸ updated_at Ã¤ldre men metadata skiljer sig â€“ synkar Ã¤ndÃ¥ {record_id}")
                                else:
                                    mark_as_processed(local_conn, change[0])
                                    return
                            else:
                                mark_as_processed(local_conn, change[0])
                                return
                except Exception:
                    pass

            if table_name == "contact" and "metadata" in data:
                local_meta = safe_json_load(data["metadata"])
                cur.execute("SELECT metadata FROM contact WHERE id = %s", (data["id"],))
                row = cur.fetchone()
                if row:
                    remote_meta = safe_json_load(row[0])
                    if remote_meta == local_meta:
                        mark_as_processed(local_conn, change[0])
                        return

            columns = ', '.join(data.keys())
            placeholders = ', '.join(['%s'] * len(data))
            values = list(data.values())
            update_keys = [k for k in data.keys() if k != 'id']
            update_set = ', '.join([f"{k} = %s" for k in update_keys])
            update_values = [data[k] for k in update_keys]
            update_values.append(record_id)
            cur.execute(
                f"UPDATE {table_name} SET {update_set} WHERE id = %s",
                update_values
            )
            if cur.rowcount == 0:
                print(f"âŒ UPDATE pÃ¥verkar 0 rader fÃ¶r {table_name} {record_id}")
                print(f"ğŸ” FÃ¶rsÃ¶ker DELETE + INSERT som fallback fÃ¶r {table_name} {record_id}")
                cur.execute(f"DELETE FROM {table_name} WHERE id = %s", (record_id,))
                columns = ', '.join(data.keys())
                placeholders = ', '.join(['%s'] * len(data))
                values = list(data.values())
                cur.execute(f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})", values)
                print(f"âœ… Fallback INSERT klar fÃ¶r {table_name} {record_id}")
                with local_conn.cursor() as local_cur:
                    local_cur.execute(
                        "INSERT INTO event_log (id, source, event_type, payload, received_at) VALUES (gen_random_uuid(), %s, %s, %s, now())",
                        (
                            'sync',
                            'sync_forced_rewrite',
                            json.dumps({
                                "record_id": str(record_id),
                                "email": data.get("booking_email"),
                                "metadata_after": data.get("metadata")
                            })
                        )
                    )
                local_conn.commit()
                print(f"ğŸ’¾ commit() efter event_log skrivning fÃ¶r {table_name} {record_id}")
                conn.commit()
                print(f"ğŸ’¾ commit() efter fallback INSERT fÃ¶r {table_name} {record_id}")
            else:
                print(f"âœ… UPDATE pÃ¥verkar {cur.rowcount} rad(er) fÃ¶r {table_name} {record_id}")
            # Kontrollera att Ã¤ndring verkligen slog igenom
            if table_name == 'contact' and 'metadata' in data:
                cur.execute(f"SELECT metadata FROM {table_name} WHERE id = %s", (record_id,))
                verify_row = cur.fetchone()
                if verify_row and not metadata_equal(verify_row[0], data.get("metadata")):
                    print(f"âŒ Molndatabas uppdaterades inte korrekt fÃ¶r {record_id}!")
                    with local_conn.cursor() as local_cur:
                        local_cur.execute(
                            "INSERT INTO event_log (id, source, event_type, payload, received_at) VALUES (gen_random_uuid(), %s, %s, %s, now())",
                            (
                                'sync',
                                'sync_mismatch_contact',
                                json.dumps({
                                    "record_id": str(record_id),
                                    "email": data.get("booking_email"),
                                    "metadata_before": verify_row[0],
                                    "metadata_after": data.get("metadata"),
                                    "diff_summary": [k for k in data.get("metadata", {}) if data["metadata"].get(k) != verify_row[0].get(k)]
                                })
                            )
                        )
                    local_conn.commit()
                else:
                    print(f"âœ… Verifierad update i molndatabasen fÃ¶r {record_id}")
            print(f"âœ… UPDATE kÃ¶rd fÃ¶r {table_name} {record_id}")
            cur.execute("SELECT metadata, updated_at FROM contact WHERE id = %s", [payload["id"]])
            updated_row = cur.fetchone()
        elif operation == 'DELETE':
            cur.execute(f"DELETE FROM {table_name} WHERE id = %s", (record_id,))
            print(f"ğŸ—‘ï¸ Raderade post {record_id} frÃ¥n {table_name}")
            print(f"ğŸ—‘ï¸ Raderade post {record_id} frÃ¥n {table_name}")
        conn.commit()
        mark_as_processed(local_conn, change[0])

def sync():
    import traceback
    local_conn = connect_db(LOCAL_DB_CONFIG)
    remote_conn = connect_db(REMOTE_DB_CONFIG)

    changes = fetch_pending_changes(local_conn)
    count = 0
    for change in changes:
        try:
            apply_change(remote_conn, change, local_conn)
            count += 1
        except Exception as e:
            print(f"âŒ Misslyckades att applicera Ã¤ndring pÃ¥ {change[1]} (id={change[2]}): {e}")
            traceback.print_exc()
    
    local_conn.close()
    remote_conn.close()

if __name__ == "__main__":
    sync()
END: sync_to_cloud.py

====================
ğŸ“„ Fil: Documents/KLR_AI/Projekt_MacSpot/macspot-api/sync_static_tables.py
ğŸ“‚ Kodtyp: ğŸ“„ Ã–vrigt
ğŸ—‚ Filtyp: ğŸ Python
ğŸ“… Senast Ã¤ndrad: 2025-04-24 17:01:52
ğŸ“ Antal rader: 61
ğŸ§© Antal funktioner: 0
ğŸ’¬ KommentarstÃ¤ckning: 0 rader (0.0%)
ğŸ“¥ Imports: 2 â€“ ['import psycopg2', 'import json']
ğŸ” LÃ¤ngsta funktion: 0 rader
ğŸ§  KomplexitetspoÃ¤ng: 6
ğŸ§ª TODO/FIXME: 0
====================
START: sync_static_tables.py
from config import LOCAL_DB_CONFIG, REMOTE_DB_CONFIG
import psycopg2
import json
from datetime import datetime


TABLES = ["translation", "booking_settings"]


def connect_db(config):
    return psycopg2.connect(**config)


def fetch_all_from_local(conn, table):
    with conn.cursor() as cur:
        cur.execute(f"SELECT * FROM {table}")
        colnames = [desc[0] for desc in cur.description]
        rows = cur.fetchall()
        return colnames, rows


def clear_remote_table(conn, table):
    with conn.cursor() as cur:
        cur.execute(f"DELETE FROM {table}")
        conn.commit()


def insert_to_remote(conn, table, columns, rows):
    with conn.cursor() as cur:
        placeholders = ', '.join(['%s'] * len(columns))
        colnames = ', '.join(columns)
        for row in rows:
            # Hantera jsonb-vÃ¤rden som json-strÃ¤ngar
            formatted_row = []
            for i, col in enumerate(columns):
                value = row[i]
                if table == 'booking_settings' and col == 'value':
                    formatted_row.append(json.dumps(value))
                else:
                    formatted_row.append(value)
            cur.execute(f"INSERT INTO {table} ({colnames}) VALUES ({placeholders})", formatted_row)
        conn.commit()


def sync_static_tables():
    local_conn = connect_db(LOCAL_DB_CONFIG)
    remote_conn = connect_db(REMOTE_DB_CONFIG)

    for table in TABLES:
        print(f"\nâ³ Synkar tabell: {table}...")
        columns, rows = fetch_all_from_local(local_conn, table)
        clear_remote_table(remote_conn, table)
        insert_to_remote(remote_conn, table, columns, rows)
        print(f"âœ… Klar med tabell: {table} ({len(rows)} rader)")

    local_conn.close()
    remote_conn.close()


if __name__ == "__main__":
    sync_static_tables()

END: sync_static_tables.py

====================
ğŸ“„ Fil: Documents/KLR_AI/Projekt_MacSpot/macspot-api/sync.py
ğŸ“‚ Kodtyp: ğŸ“„ Ã–vrigt
ğŸ—‚ Filtyp: ğŸ Python
ğŸ“… Senast Ã¤ndrad: 2025-06-02 17:06:12
ğŸ“ Antal rader: 114
ğŸ§© Antal funktioner: 0
ğŸ’¬ KommentarstÃ¤ckning: 0 rader (0.0%)
ğŸ“¥ Imports: 2 â€“ ['import psycopg2', 'import json']
ğŸ” LÃ¤ngsta funktion: 0 rader
ğŸ§  KomplexitetspoÃ¤ng: 15
ğŸ§ª TODO/FIXME: 0
====================
START: sync.py
import psycopg2
import json
from datetime import datetime
from config import LOCAL_DB_CONFIG

def safe_json_load(data, default={}):
    try:
        return json.loads(data) if isinstance(data, str) else data
    except Exception:
        return default

def metadata_equal(meta1, meta2):
    m1 = safe_json_load(meta1)
    m2 = safe_json_load(meta2)
    return m1 == m2

# Anslutning till lokal PostgreSQL
conn = psycopg2.connect(**LOCAL_DB_CONFIG)
cursor = conn.cursor()

# Rensa Ã¤ldre UPDATE-rader (endast senaste behÃ¶vs per record_id)
cursor.execute("""
    DELETE FROM pending_changes pc
    WHERE operation = 'UPDATE'
      AND processed = false
      AND direction = 'out'
      AND id NOT IN (
        SELECT id FROM (
          SELECT id,
                 ROW_NUMBER() OVER (PARTITION BY record_id ORDER BY created_at DESC) AS rn
          FROM pending_changes
          WHERE operation = 'UPDATE'
            AND processed = false
            AND direction = 'out'
        ) sub
        WHERE rn = 1
      );
""")

# HÃ¤mta EN Ã¤ndring per kontakt (record_id) â€“ endast senaste per kontakt exporteras med hjÃ¤lp av ROW_NUMBER()
cursor.execute("""
    SELECT id, table_name, record_id, operation, payload, created_at
    FROM (
        SELECT *, ROW_NUMBER() OVER (PARTITION BY record_id ORDER BY created_at DESC) as rn
        FROM pending_changes
        WHERE processed = false AND direction = 'out'
    ) sub
    WHERE rn = 1
""")

rows = cursor.fetchall()

# Filtrera bort poster dÃ¤r metadata Ã¤r identisk med befintlig kontakt
filtered_rows = []
for row in rows:
    change_id, table, record_id, operation, payload, created_at = row
    data = json.loads(payload) if isinstance(payload, str) else payload
    if table == "contact" and operation == "UPDATE":
        try:
            cursor.execute("SELECT metadata FROM contact WHERE id = %s", (record_id,))
            result = cursor.fetchone()
            if result:
                incoming_metadata = data.get("metadata")
                if metadata_equal(result[0], incoming_metadata):
                    continue
        except Exception as e:
            print(f"âš ï¸ Kunde inte jÃ¤mfÃ¶ra metadata fÃ¶r {record_id}: {e}")
    filtered_rows.append(row)
rows = filtered_rows

# Skapa exportformat
export = []
for row in rows:
    change_id, table, record_id, operation, payload, created_at = row
    export.append({
        "change_id": str(change_id),
        "table": table,
        "operation": operation,
        "data": payload
    })

# Logga diff fÃ¶r contact-uppdateringar innan exporten skrivs till disk
for entry in export:
    if entry["table"] == "contact" and entry["operation"] == "UPDATE":
        try:
            cursor.execute("SELECT metadata FROM contact WHERE id = %s", (entry["data"]["id"],))
            existing = cursor.fetchone()
            if existing and not metadata_equal(existing[0], entry["data"].get("metadata")):
                cursor.execute("""
                    INSERT INTO event_log (id, source, event_type, payload, received_at)
                    VALUES (gen_random_uuid(), %s, %s, %s, now())
                """, (
                    'sync',
                    'sync_local_diff',
                    json.dumps({
                        "record_id": str(entry["data"]["id"]),
                        "email": entry["data"].get("booking_email"),
                        "metadata_before": existing[0],
                        "metadata_after": entry["data"].get("metadata"),
                        "diff_summary": [k for k in entry["data"].get("metadata", {}) if entry["data"]["metadata"].get(k) != existing[0].get(k)]
                    })
                ))
        except Exception as e:
            print(f"âš ï¸ Kunde inte logga diff fÃ¶r contact {entry['data'].get('id')}: {e}")

# Spara till JSON-fil med tidsstÃ¤mpel
if export:
    first_type = export[0]["table"] if export else "unknown"
    filename = f"sync_outbox/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{first_type}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(export, f, indent=2, ensure_ascii=False)

cursor.close()
conn.close()

END: sync.py

====================
ğŸ“„ Fil: Documents/KLR_AI/Projekt_MacSpot/macspot-api/sync_all.py
ğŸ“‚ Kodtyp: ğŸ“„ Ã–vrigt
ğŸ—‚ Filtyp: ğŸ Python
ğŸ“… Senast Ã¤ndrad: 2025-06-02 17:07:11
ğŸ“ Antal rader: 213
ğŸ§© Antal funktioner: 0
ğŸ’¬ KommentarstÃ¤ckning: 0 rader (0.0%)
ğŸ“¥ Imports: 7 â€“ ['import os', 'import subprocess', 'import sys', 'import psycopg2', 'import json', 'import socket', 'import traceback']
ğŸ” LÃ¤ngsta funktion: 0 rader
ğŸ§  KomplexitetspoÃ¤ng: 25
ğŸ§ª TODO/FIXME: 0
====================
START: sync_all.py
BASE = "/Users/danielkallberg/Documents/KLR_AI/Projekt_MacSpot/macspot-api"


import os
import subprocess
from datetime import datetime, timezone
import sys
import psycopg2
import json

with open("/tmp/launchd_debug.txt", "a") as f:
    f.write("[sync_all.py] KÃ¶rning initierad\n")

with open("/tmp/env_debug.txt", "w") as f:
    f.write("START\n")
    f.write(str(dict(os.environ)))

# Delad json- och metadata-funktionalitet som anvÃ¤nds i flera synkmoduler
def safe_json_load(data, default={}):
    try:
        return json.loads(data) if isinstance(data, str) else data
    except Exception:
        return default

def metadata_equal(meta1, meta2):
    m1 = safe_json_load(meta1)
    m2 = safe_json_load(meta2)
    return m1 == m2

log_dir = "/Users/danielkallberg/Documents/KLR_AI/Projekt_MacSpot"
log_out = os.path.join(log_dir, "macspot_sync.log")
log_err = os.path.join(log_dir, "macspot_sync_error.log")

# Se till att loggfilerna existerar
for path in [log_out, log_err]:
    if not os.path.exists(path):
        with open(path, 'w'):
            pass

# Skriv ut manuell/automatisk kÃ¶rningsinfo till loggen
is_manual = os.environ.get("LAUNCHD_RUN") != "true"
log_mode = "MANUELL" if is_manual else "AUTOMATISK (launchd)"
with open("/tmp/env_debug.txt", "a") as f:
    f.write(f"LAUNCHD_RUN: {os.environ.get('LAUNCHD_RUN')}\n")
# Debug environment variables to file
with open("/tmp/env_debug.txt", "w") as f:
    f.write(str(dict(os.environ)))
out = open(log_out, 'a')
err = open(log_err, 'a')
sys.stdout = out
sys.stderr = err
print(f"ğŸš€ KÃ¶rlÃ¤ge: {log_mode} â€“ {datetime.now(timezone.utc).isoformat()}")

def run_script(name, script_path):
    subprocess.run(["/Users/danielkallberg/Documents/KLR_AI/venv/bin/python", f"{BASE}/{script_path}"], check=True)

try:
    start_time = datetime.now(timezone.utc)
    def is_database_online(host, port):
        import socket
        try:
            socket.create_connection((host, port), timeout=2)
            return True
        except:
            return False

    def run_healthcheck():
        try:
            subprocess.run(
                ["/Users/danielkallberg/Documents/KLR_AI/venv/bin/python", f"{BASE}/healthcheck_sync.py"],
                check=True
            )
        except Exception as e:
            print(f"âŒ Healthcheck misslyckades: {e}")

    # Kontrollera att bÃ¥da databaser Ã¤r online innan sync startar
    if not is_database_online("localhost", 5433):
        print("âŒ Lokal databas Ã¤r inte tillgÃ¤nglig (localhost:5433)")
        exit(1)

    if not is_database_online("macspotpg.postgres.database.azure.com", 5432):
        print("âŒ Azure-databasen Ã¤r inte tillgÃ¤nglig (macspotpg.postgres.database.azure.com:5432)")
        exit(1)

    print(f"ğŸ“Œ KÃ¶rning initierad: {datetime.now(timezone.utc).isoformat()}")

    print("ğŸ§ª KÃ¶r healthcheck_sync.py...")
    run_healthcheck()

    print(f"\nğŸ”„ [{datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}] Startar fullstÃ¤ndig synk...")

    scripts_part1 = [
        ("ğŸŸ¡ KÃ¶r sync.py...", "sync.py"),
        ("ğŸŸ¢ KÃ¶r sync_to_cloud.py...", "sync_to_cloud.py")
    ]

    for msg, script in scripts_part1:
        run_script(msg, script)

    scripts_part2 = [
        ("ğŸ”µ KÃ¶r sync_from_cloud.py...", "sync_from_cloud.py")
    ]

    for msg, script in scripts_part2:
        run_script(msg, script)

    today_prefix = datetime.now(timezone.utc).strftime('%Y%m%d')
    outbox_dir = os.path.join(BASE, 'sync_outbox')
    files = [f for f in os.listdir(outbox_dir) if f.startswith(today_prefix)]
    files_with_type = [f for f in files if len(f.split("_")) >= 3]
    num_changes = len(files_with_type)

    if num_changes == 0:
        print("â„¹ï¸ Ingen fÃ¶rÃ¤ndring hittades att synka.")
        print("ğŸ“­ Inga fler Ã¤ndringar kvar i pending_changes.")
    else:
        print(f"ğŸ“¤ Totalt {num_changes} Ã¤ndring(ar) skickades till molnet:")
        files = [f for f in sorted(os.listdir(outbox_dir)) if f.startswith(today_prefix)]
        summary = {}
        for f in files:
            parts = f.split("_")
            if len(parts) >= 3:
                typ = parts[2].split(".")[0]
                summary[typ] = summary.get(typ, 0) + 1

        if summary:
            print("ğŸ§¾ Sammanfattning per typ:")
            for typ, count in summary.items():
                print(f"   â€¢ {typ}: {count} st")

        # --- Kontrollera och skriv ut Ã¤ldre JSON-filer i sync_outbox ---
        old_files = [f for f in os.listdir(outbox_dir) if not f.startswith(today_prefix)]
        if old_files:
            print("ğŸ“‚ Ã„ldre JSON-filer som ligger kvar i sync_outbox:")
            for f in old_files:
                print(f"   â€¢ {f}")

        print("ğŸ“Š Kontroll av Ã¥terstÃ¥ende Ã¤ndringar i pending_changes...")

        # Lokalt
        local = psycopg2.connect(
            dbname="macspot",
            user="postgres",
            host="localhost",
            port=5433
        )
        cur_local = local.cursor()
        cur_local.execute("""
            SELECT COUNT(*) FROM pending_changes
            WHERE direction = 'out' AND processed = false
        """)
        out_local = cur_local.fetchone()[0]
        cur_local.execute("""
            SELECT COUNT(*) FROM pending_changes
            WHERE direction = 'out' AND processed = false
            GROUP BY record_id
        """)
        print(f"   â€¢ Lokalt â†’ molnet: {out_local} Ã¤ndring(ar) kvar Ã¶ver {cur_local.rowcount} kontakt(er).")
        cur_local.close()
        local.close()

        # Molnet
        cloud = psycopg2.connect(
            dbname="postgres",
            user="daniel",
            host="macspotpg.postgres.database.azure.com",
            port=5432
        )
        cur_cloud = cloud.cursor()
        cur_cloud.execute("""
            SELECT COUNT(*) FROM pending_changes
            WHERE direction = 'out' AND processed = false
        """)
        out_cloud = cur_cloud.fetchone()[0]
        cur_cloud.execute("""
            SELECT COUNT(*) FROM pending_changes
            WHERE direction = 'out' AND processed = false
            GROUP BY record_id
        """)
        cur_cloud.close()
        cloud.close()

    print(f"\nâœ… [{datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}] FullstÃ¤ndig synk kÃ¶rd.")

    # Sammanfatta diff-loggar
    with psycopg2.connect(dbname="macspot", user="postgres", host="localhost", port=5433) as conn:
        with conn.cursor() as cur:
            cur.execute("""
                SELECT event_type, COUNT(*) 
                FROM event_log 
                WHERE received_at > now() - interval '10 minutes' 
                  AND event_type IN ('sync_local_diff', 'sync_mismatch_contact', 'sync_fromcloud_mismatch')
                GROUP BY event_type
            """)
            results = cur.fetchall()
            if results:
                print("ğŸ§¾ Diff-loggar skapade (senaste 10 min):")
                for event_type, count in results:
                    print(f"   â€¢ {event_type}: {count}")
            else:
                print("ğŸ§¾ Inga diff-loggar skapades under senaste 10 minuter.")

except Exception as e:
    import traceback
    print("âŒ Ett ovÃ¤ntat fel intrÃ¤ffade under kÃ¶rningen:")
    print(traceback.format_exc())

finally:
    print(f"ğŸ KÃ¶rning avslutad: {datetime.now(timezone.utc).isoformat()}")
    duration = datetime.now(timezone.utc) - start_time
    print(f"â±ï¸ Total kÃ¶rtid: {int(duration.total_seconds())} sekunder")
    out.close()
    err.close()
END: sync_all.py

====================
ğŸ“„ Fil: Documents/KLR_AI/Projekt_MacSpot/macspot-api/sync_plist.xml
ğŸ“‚ Kodtyp: ğŸ“„ Ã–vrigt
ğŸ—‚ Filtyp: ğŸ“„ OkÃ¤nt format
ğŸ“… Senast Ã¤ndrad: 2025-06-02 16:22:08
ğŸ“ Antal rader: 26
ğŸ§© Antal funktioner: 0
ğŸ’¬ KommentarstÃ¤ckning: 0 rader (0.0%)
ğŸ“¥ Imports: 0 â€“ Inga
ğŸ” LÃ¤ngsta funktion: 0 rader
ğŸ§  KomplexitetspoÃ¤ng: 0
ğŸ§ª TODO/FIXME: 0
====================
START: sync_plist.xml
cat > ~/Library/LaunchAgents/com.macspot.sync.plist <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN"
  "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
  <key>Label</key>
  <string>com.macspot.sync</string>
  <key>ProgramArguments</key>
  <array>
    <string>/Users/danielkallberg/Documents/KLR_AI/venv/bin/python3</string>
    <string>/Users/danielkallberg/Documents/KLR_AI/Projekt_MacSpot/macspot-api/sync_all.py</string>
  </array>
  <key>WorkingDirectory</key>
  <string>/Users/danielkallberg/Documents/KLR_AI/Projekt_MacSpot/macspot-api</string>
  <key>StartInterval</key>
  <integer>300</integer>
  <key>RunAtLoad</key>
  <true/>
  <key>StandardOutPath</key>
  <string>/tmp/macspot_sync.log</string>
  <key>StandardErrorPath</key>
  <string>/tmp/macspot_sync_error.log</string>
</dict>
</plist>
EOF
END: sync_plist.xml

====================
ğŸ“„ Fil: Library/LaunchAgents/com.macspot.sync.plist
ğŸ“‚ Kodtyp: ğŸ“„ Ã–vrigt
ğŸ—‚ Filtyp: ğŸ“„ OkÃ¤nt format
ğŸ“… Senast Ã¤ndrad: 2025-06-02 16:30:38
ğŸ“ Antal rader: 23
ğŸ§© Antal funktioner: 0
ğŸ’¬ KommentarstÃ¤ckning: 0 rader (0.0%)
ğŸ“¥ Imports: 0 â€“ Inga
ğŸ” LÃ¤ngsta funktion: 0 rader
ğŸ§  KomplexitetspoÃ¤ng: 0
ğŸ§ª TODO/FIXME: 0
====================
START: com.macspot.sync.plist
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>Label</key>
	<string>com.macspot.sync</string>
	<key>ProgramArguments</key>
	<array>
		<string>/Users/danielkallberg/Documents/KLR_AI/venv/bin/python3</string>
		<string>/Users/danielkallberg/Documents/KLR_AI/Projekt_MacSpot/macspot-api/sync_all.py</string>
	</array>
	<key>RunAtLoad</key>
	<true/>
	<key>StandardErrorPath</key>
	<string>/tmp/macspot_sync_error.log</string>
	<key>StandardOutPath</key>
	<string>/tmp/macspot_sync.log</string>
	<key>StartInterval</key>
	<integer>300</integer>
	<key>WorkingDirectory</key>
	<string>/Users/danielkallberg/Documents/KLR_AI/Projekt_MacSpot/macspot-api</string>
</dict>
</plist>

END: com.macspot.sync.plist

====================
ğŸ“„ Fil: Documents/KLR_AI/Projekt_MacSpot/macspot-api/healthcheck_sync.py
ğŸ“‚ Kodtyp: ğŸ“„ Ã–vrigt
ğŸ—‚ Filtyp: ğŸ Python
ğŸ“… Senast Ã¤ndrad: 2025-06-02 17:17:33
ğŸ“ Antal rader: 80
ğŸ§© Antal funktioner: 0
ğŸ’¬ KommentarstÃ¤ckning: 0 rader (0.0%)
ğŸ“¥ Imports: 2 â€“ ['import psycopg2', 'import sys']
ğŸ” LÃ¤ngsta funktion: 0 rader
ğŸ§  KomplexitetspoÃ¤ng: 6
ğŸ§ª TODO/FIXME: 0
====================
START: healthcheck_sync.py
# ğŸ“„ FÃ¶rbÃ¤ttrad version av healthcheck_sync.py

import psycopg2
from config import LOCAL_DB_CONFIG, REMOTE_DB_CONFIG
from datetime import datetime, timezone
import sys

__version__ = "1.0.1"

def get_pending_count(conn, label):
    with conn.cursor() as cur:
        cur.execute("""
            SELECT COUNT(*) FROM pending_changes
            WHERE direction = 'out' AND processed = false
        """)
        count = cur.fetchone()[0]
        print(f"ğŸ” {label}: {count} osynkade fÃ¶rÃ¤ndringar")
        return count

def check_db_connection(name, config):
    try:
        start = datetime.now(timezone.utc)
        conn = psycopg2.connect(**config)
        latency = (datetime.now(timezone.utc) - start).total_seconds()
        print(f"âœ… Anslutning till {name} OK (latens: {latency:.2f} sek)")
        return conn
    except Exception as e:
        print(f"âŒ Fel vid anslutning till {name}: {e}")
        return None

def main():
    print(f"\nğŸ“‹ Healthcheck MacSpot sync v{__version__} â€“ {datetime.now(timezone.utc).isoformat()} UTC\n")
    errors = 0

    local_conn = check_db_connection("Lokal databas", LOCAL_DB_CONFIG)
    if local_conn:
        get_pending_count(local_conn, "Lokal â†’ moln")
        local_conn.close()
    else:
        errors += 1

    remote_conn = check_db_connection("Molndatabas", REMOTE_DB_CONFIG)
    if remote_conn:
        get_pending_count(remote_conn, "Moln â†’ lokal")
        remote_conn.close()
    else:
        errors += 1

    # HÃ¤mta senaste mismatch-loggar frÃ¥n event_log
    try:
        with psycopg2.connect(**LOCAL_DB_CONFIG) as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT event_type, payload->>'email', payload->>'diff_summary', received_at
                    FROM event_log
                    WHERE received_at > now() - interval '10 minutes'
                      AND event_type IN (
                          'sync_local_diff', 'sync_mismatch_contact', 'sync_fromcloud_mismatch', 'sync_forced_rewrite'
                      )
                    ORDER BY received_at DESC
                    LIMIT 10
                """)
                rows = cur.fetchall()
                if rows:
                    print("\nğŸš¨ Senaste synkavvikelser:")
                    for event_type, email, diff, timestamp in rows:
                        print(f"â€¢ {event_type}: {email} â€“ {diff} @ {timestamp}")
                else:
                    print("\nâœ… Inga mismatch-loggar i event_log senaste 10 minuterna.")
    except Exception as e:
        print(f"âš ï¸ Kunde inte lÃ¤sa mismatch-loggar: {e}")

    if errors > 0:
        print(f"\nâŒ Healthcheck avslutades med {errors} fel.\n")
        sys.exit(1)
    else:
        print(f"\nâœ… Healthcheck genomfÃ¶rd utan fel.\n")

if __name__ == "__main__":
    main()
END: healthcheck_sync.py

ğŸ“ KONFIGURATIONSFILER (function.json / host.json / package.json / .funcignore)
====================================

